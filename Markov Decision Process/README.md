A Markov decision process (MDP) refers to a stochastic decision-making process that uses a mathematical framework to model the decision-making of a dynamic system. It is used in scenarios where the results are either random or controlled by a decision maker, which makes sequential decisions over time.


Markov State is similar to a Decision Tree Chance node, but unlike a Decision tree chance node, a Markov state can be cyclic. That means a State can transition to another state and come back to the same state. You can add as many states as you want from this interface and proceed

In a Markov Decision Process, there are also 3 important properties that must be satisfied:

1. The environment is fully observable. This means that all the stations are shown on the Tube map.

2. The future is just dependent on the present.

3. The probability to reach the successor state only depends on the current state.


The four components of an MDP model are: a set of states, a set of actions, the effects of the actions and the immediate value of the actions.
